slice_max(n)
xc_women
View(xc_women)
View(xc_men)
xc_men <- getElement(html_table(html_elements(read_html(url1), "table")), 6)
View(xc_men)
# YOUR CODE HERE
24 +11+2+7
# YOUR CODE HERE
34/44
# YOUR CODE HERE
30/44
comb <- left_join(vac_mut, meas_mut, by = c("iso3", "Year")) %>%
filter(iso3 == c("AFG", "CIV", "GBR")) %>%
filter(Year == ends_with("0", "5"))
View(comb)
comb <- left_join(vac_mut, meas_mut, by = c("iso3", "Year")) %>%
filter(iso3 == c("AFG", "CIV", "GBR"))
xc_women %>%
mutate("team_total_score")
25/44
xc_women %>%
mutate("team_total_score", "team_rank")
xc_men %>%
mutate("team_total_score", "team_rank")
na.omit(xc_women, xc_men)
xc_women <- getElement(html_table(html_elements(read_html(url1), "table")), 4)[-c(2,3,5,6)]
View(xc_women)
xc_men <- getElement(html_table(html_elements(read_html(url1), "table")), 6)[-c(2,3,5,6)]
View(xc_men)
na.omit(xc_women, xc_men)
xc_women %>%
mutate("team_total_score", "team_rank")
xc_men %>%
mutate("team_total_score", "team_rank")
na.omit(xc_women, xc_men)
xc_men %>%
mutate("team_total_score", "team_rank")
gplot(data = eng, aes(x = Gender)) + geom_bar() + facet_wrap(~Stream)
gplot(data = eng, aes(x = Gender,)) + geom_bar() + facet_wrap(~Stream)
ggplot(data = eng, aes(x = Gender)) + geom_bar() + facet_wrap(~Stream)
(4/28)/(13/28)
(13/15) * (15/28) + (4/13) * (15/28)
(13/15) * (15/28) + (4/13) * (13/28)
((13/15) * (15/28))/((13/15) * (15/28) + (4/13) * (13/28))
library(tidyverse)
library(tidymodels)
library("tidyverse")
library("tidymodels")
install.packages("tidyverse")
install.packages(c("tidyverse", "tidymodels"))
library(tidyverse)
library(tidymodels)
library(tidyverse)
library(tidymodels)
library(tidyverse)
library(tidyverse)
library(tidymodels)
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(patchwork)
library(DataExplorer)
library(glmnet)
library(rpart)
library(ranger)
library(bonsai)
library(lightgbm)
test_file <- vroom("test.csv")
test_file <- vroom("test.csv")
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(patchwork)
library(DataExplorer)
library(glmnet)
library(rpart)
library(ranger)
library(bonsai)
library(lightgbm)
test_file <- vroom("test.csv")
train_file <- vroom("train.csv")
library(vroom)
test_file <- vroom("test.csv")
setwd("C:/Users/sophi/OneDrive/Documents/Stat_348/KaggleBikeShare")
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(patchwork)
library(DataExplorer)
library(glmnet)
library(rpart)
library(ranger)
library(bonsai)
library(lightgbm)
test_file <- vroom("test.csv")
train_file <- vroom("train.csv")
train_file <- train_file %>%
select(-registered, -casual)
train_file <- train_file %>%
mutate(count = log(count))
## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% #Type of model
set_engine("lm") %>% # Engine = What R function to use
set_mode("regression") # Regression just means quantitative response
my_recipe <- recipe(count ~ ., data = train_file) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = as.factor(weather)) %>%
step_mutate(season = as.factor(season)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)) %>%
step_date(datetime, features =  "doy") %>%  # add day-of-week, day-of-year
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe)
baked_recipie <- bake(prepped_recipe, new_data = train_file)
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>%
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Set up grid of tuning values
mygrid <- grid_regular(mtry(range = c(1, 15)),min_n(), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 5, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Set up grid of tuning values
mygrid <- grid_regular(mtry(range = c(1, 15)), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 5, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Set up grid of tuning values
mygrid <- grid_regular(trees(range = c(1, 15)), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 5, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_file)
## Predict
final_wf %>%
predict(new_data = test_file)
## Run all the steps on test data
lin_preds <- predict(final_wf, new_data = test_file )
lin_preds <- lin_preds %>%
mutate(.pred = exp(.pred))
kaggle_submission <- lin_preds %>%
bind_cols(., test_file) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./TuningRandomForestModel.csv", delim=",")
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>%
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Set up grid of tuning values
mygrid <- grid_regular(trees(range = c(1, 15)), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 7, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_file)
## Predict
final_wf %>%
predict(new_data = test_file)
## Run all the steps on test data
lin_preds <- predict(final_wf, new_data = test_file )
lin_preds <- lin_preds %>%
mutate(.pred = exp(.pred))
kaggle_submission <- lin_preds %>%
bind_cols(., test_file) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./TuningRandomForestModel.csv", delim=",")
plot(dbeta(.5, 1, 1))
plot(dbeta(.7, 3, 7))
plot(dbeta(.5, 1, 1), line - "l")
plot(dbeta(.5, 1, 1), line - "h")
plot(dbeta(.5, 1, 1), line = "h")
plot(dbeta(.5, 1, 1), line = "l")
# Sequence of x values between 0 and 1
x <- seq(0, 1, length.out = 100)
# Compute density values
y <- dbeta(x, shape1 = 1, shape2 = 1)
# Plot
plot(x, y, type = "l", lwd = 2, col = "blue",
main = "Beta(1,1) PDF",
ylab = "Density", xlab = "x")
# Compute density values
y <- dbeta(x, shape1 = 3, shape2 = 7)
# Plot
plot(x, y, type = "l", lwd = 2, col = "blue",
main = "Beta(1,1) PDF",
ylab = "Density", xlab = "x")
# Plot
plot(x, y, type = "l", lwd = 2, col = "orange",
main = "Beta(1,1) PDF",
ylab = "Density", xlab = "x")
# Compute density values
y <- dbeta(x, shape1 = 100, shape2 = 74)
# Plot
plot(x, y, type = "l", lwd = 2, col = "orange",
main = "Beta(1,1) PDF",
ylab = "Density", xlab = "x")
# Compute density values
y <- dbeta(x, shape1 = .2, shape2 = 5)
# Plot
plot(x, y, type = "l", lwd = 2, col = "orange",
main = "Beta(1,1) PDF",
ylab = "Density", xlab = "x")
# Compute density values
y <- dbeta(x, shape1 = .5, shape2 = .5)
# Plot
plot(x, y, type = "l", lwd = 2, col = "orange",
main = "Beta(1,1) PDF",
ylab = "Density", xlab = "x")
var1<-
mean(dbeta(x,1,1))
# Sequence of x values between 0 and 1
x <- seq(0, 1, length.out = 100)
var1<-
mean(dbeta(x,1,1))
var1<-
mean1 <- mean(dbeta(x,1,1))
mean1
mean1 <- 1/(1+1)
var1 <- (1*1)/(((1*1)^2)*(1+1+1))
var1
var1 <- (1*1)/(((1+1)^2)*(1+1+1))
var1
mode1 <- (1-1)/(1+1-2)
mean2 <- 3/(3+7)
var2 <- (3*7)/(((3+7)^2)*(3+7+1))
mode2 <- (3-1)/(3+7-2)
mean3 <- 100/(100+74)
var3 <- (100*74)/(((100+74)^2)*(100+74+1))
mode3 <- (100-1)/(100+74-2)
mean2
var2
mode2
mean3
var3
mode3
setwd("C:/Users/sophi/OneDrive/Documents/Stat_348/KaggleBikeShare")
library(tidyverse)
library(tidymodels)
library(vroom)
library(ggplot2)
library(patchwork)
library(DataExplorer)
library(glmnet)
library(rpart)
library(ranger)
library(bonsai)
library(lightgbm)
test_file <- vroom("test.csv")
train_file <- vroom("train.csv")
train_file <- train_file %>%
select(-registered, -casual)
train_file <- train_file %>%
mutate(count = log(count))
## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% #Type of model
set_engine("lm") %>% # Engine = What R function to use
set_mode("regression") # Regression just means quantitative response
my_recipe <- recipe(count ~ ., data = train_file) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = as.factor(weather)) %>%
step_mutate(season = as.factor(season)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)) %>%
step_date(datetime, features =  "doy") %>%  # add day-of-week, day-of-year
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe)
baked_recipie <- bake(prepped_recipe, new_data = train_file)
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>%
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Set up grid of tuning values
mygrid <- grid_regular(trees(range = c(1, 15)), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 5, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_file)
## Predict
final_wf %>%
predict(new_data = test_file)
## Run all the steps on test data
lin_preds <- predict(final_wf, new_data = test_file )
lin_preds <- lin_preds %>%
mutate(.pred = exp(.pred))
kaggle_submission <- lin_preds %>%
bind_cols(., test_file) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./TuningRandomForestModel.csv", delim=",")
## Split data for CV
folds <- vfold_cv(train_file, v = 4, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_file)
## Predict
final_wf %>%
predict(new_data = test_file)
## Run all the steps on test data
lin_preds <- predict(final_wf, new_data = test_file )
lin_preds <- lin_preds %>%
mutate(.pred = exp(.pred))
kaggle_submission <- lin_preds %>%
bind_cols(., test_file) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./TuningRandomForestModel.csv", delim=",")
baked_recipie
my_recipe <- recipe(count ~ ., data = train_file) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = as.factor(weather)) %>%
step_mutate(season = as.factor(season)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)) %>%
step_mutate(hour = as.factor(datetime_hour)) %>%
step_date(datetime, features =  "doy") %>%  # add day-of-week, day-of-year
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe)
baked_recipie <- bake(prepped_recipe, new_data = train_file)
bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
set_engine("dbarts") %>%
set_mode("regression")
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model)
## Set up grid of tuning values
mygrid <- grid_regular(trees(range = c(1, 15)), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 4, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Set up grid of tuning values
mygrid <- grid_regular(trees(range = c(1, 16)), levels = 3)
## Split data for CV
folds <- vfold_cv(train_file, v = 4, repeats=1)
## Run the CV1
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=mygrid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best(metric="rmse")
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=train_file)
## Predict
final_wf %>%
predict(new_data = test_file)
## Run all the steps on test data
lin_preds <- predict(final_wf, new_data = test_file )
lin_preds <- lin_preds %>%
mutate(.pred = exp(.pred))
kaggle_submission <- lin_preds %>%
bind_cols(., test_file) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./TuningRandomForestModel.csv", delim=",")
baked_recipie
View(baked_recipie)
library(agua)
install.packages("agua")
h2o::h2o.init()
library(agua)
h2o::h2o.init()
test_file <- vroom("test.csv")
train_file <- vroom("train.csv")
train_file <- train_file %>%
select(-registered, -casual)
train_file <- train_file %>%
mutate(count = log(count))
## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% #Type of model
set_engine("lm") %>% # Engine = What R function to use
set_mode("regression") # Regression just means quantitative response
my_recipe <- recipe(count ~ ., data = train_file) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = as.factor(weather)) %>%
step_mutate(season = as.factor(season)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)) %>%
step_mutate(hour = as.factor(datetime_hour)) %>%
step_date(datetime, features =  "doy") %>%  # add day-of-week, day-of-year
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe)
baked_recipie <- bake(prepped_recipe, new_data = train_file)
## Define the model
## max_runtime_secs = how long to let h2o.ai run
## max_models = how many models to stack
auto_model <- auto_ml() %>%
set_engine("h2o", max_runtime_secs=, max_models=) %>%
set_mode("regression")
## Define the model
## max_runtime_secs = how long to let h2o.ai run
## max_models = how many models to stack
auto_model <- auto_ml() %>%
set_engine("h2o", max_runtime_secs=300, max_models=6) %>%
set_mode("regression")
## Combine my Recipe and Model into a Workflow and fit
automl_wf <- workflow() %>%
add_recipe(baked_recipie) %>%
add_model(auto_model) %>%
fit(data=train_file)
## Combine my Recipe and Model into a Workflow and fit
automl_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(auto_model) %>%
fit(data=train_file)
## Run all the steps on test data
preds <- predict(automl_wf, new_data = test_file )
preds <- preds %>%
mutate(.pred = exp(.pred))
kaggle_submission <- lin_preds %>%
bind_cols(., test_file) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./InterfacingH20.AIModel.csv", delim=",")
